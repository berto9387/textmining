{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Classification in Cross-Validation using CNB Algorithm"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import re\nimport unicodedata\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom lime import lime_text\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nfrom sklearn import preprocessing\nfrom nltk.tokenize import word_tokenize \nfrom sklearn.pipeline import make_pipeline\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n\nfrom sklearn.naive_bayes import ComplementNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Hyparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha=0.2\n\nnfold=10\n\nnumber_of_feature = 5000\n\ndataset_path = \"../input/indian-parliament/rajyasabha_questions_and_answers.xlsx\"\nsheet_name=\"rajyasabha_questions_and_answer\"\n\ntext=\"question_description\"\nreview=\"ministry\"\n\nlanguage=\"english\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel(dataset_path, sheet_name=sheet_name)\nprint(f'Found {len(df)} texts.')\n\nprint(f'{df[review].isnull().sum()} document(s) with no classification removed')\ndf=df[pd.notnull(df[review])]\n\nprint(f'{df[text].isnull().sum()} document(s) with no text removed')\ndf=df[pd.notnull(df[text])]\n\nle = preprocessing.LabelEncoder()\nle.fit(df[review])\ndf[review]=le.transform(df[review])\n\nclasses = [int(c) for c in df[review]]\ndocuments = [d for d in df[text]]\n\n###### Print dataset ###################\ndf = df[[text,review ]]\ndf.columns = ['sentiment', 'review']\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Distribution Diagram"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.bincount(classes)\nx = np.arange(len(y))\nfig, ax = plt.subplots(figsize=(10,8))\nplt.bar(x, y,width=0.7)\nax.set_xticks(x)\nax.set_aspect('auto')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Preprocessor"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessor(text):\n    text = re.sub('<[^>]*>', ' ', str(text))\n    text=re.sub('\\d+',' ',str(text))\n    text=re.sub('[ﾫﾻ]','',str(text))\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           str(text))\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' +\n            ' '.join(emoticons).replace('-', ''))\n    return text\n\ndef strip_accents(text):\n    \n    try:\n        text = unicode(text, 'utf-8')\n    except (TypeError, NameError): # unicode is a default on python 3 \n        pass\n    text = unicodedata.normalize('NFD', text)\n    text = text.encode('ascii', 'ignore')\n    text = text.decode(\"utf-8\")\n    return str(text)\n\nstop=set(stopwords.words(language))\n\ndef tokenizer_porter(text):\n    word_tokens = word_tokenize(text)\n    stemmer = SnowballStemmer(language, ignore_stopwords=True)\n    return [stemmer.stem(word) for word in word_tokens]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Infogain Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class IG():\n    def __init__(self,k_features):\n        self.k_features = k_features\n        \n\n    \n    def fit(self, X, y):\n        def _calIg():\n            entropy_x_set = 0\n            entropy_x_not_set = 0\n            for c in classCnt:\n                probs = classCnt[c] / float(featureTot)\n                entropy_x_set = entropy_x_set - probs * np.log(probs)\n                probs = (classTotCnt[c] - classCnt[c]) / float(tot - featureTot)\n                entropy_x_not_set = entropy_x_not_set - probs * np.log(probs)\n            for c in classTotCnt:\n                if c not in classCnt:\n                    probs = classTotCnt[c] / float(tot - featureTot)\n                    entropy_x_not_set = entropy_x_not_set - probs * np.log(probs)\n            return entropy_before - ((featureTot / float(tot)) * entropy_x_set\n                             +  ((tot - featureTot) / float(tot)) * entropy_x_not_set)\n        tot = X.shape[0]\n        classTotCnt = {}\n        entropy_before = 0\n        for i in y:\n            if i not in classTotCnt:\n                classTotCnt[i] = 1\n            else:\n                classTotCnt[i] = classTotCnt[i] + 1\n        for c in classTotCnt:\n            probs = classTotCnt[c] / float(tot)\n            entropy_before = entropy_before - probs * np.log(probs)\n\n        nz = X.T.nonzero()\n        pre = 0\n        classCnt = {}\n        featureTot = 0\n        information_gain = []\n        for i in range(0, len(nz[0])):\n            if (i != 0 and nz[0][i] != pre):\n                for notappear in range(pre+1, nz[0][i]):\n                    information_gain.append(0)\n                ig = _calIg()\n                information_gain.append(ig)\n                pre = nz[0][i]\n                classCnt = {}\n                featureTot = 0\n            featureTot = featureTot + 1\n            yclass = y[nz[1][i]]\n            if yclass not in classCnt:\n                classCnt[yclass] = 1\n            else:\n                classCnt[yclass] = classCnt[yclass] + 1\n        ig = _calIg()\n        information_gain.append(ig)\n        information_gain_a=np.asarray(information_gain)\n        self.indices_ = np.argsort(information_gain_a)[-self.k_features:]\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(strip_accents=strip_accents,\n                        lowercase=False,\n                        preprocessor=preprocessor,\n                        tokenizer=tokenizer_porter,\n                        stop_words=stop,\n                        min_df = 4\n                       )\n\n    \nig=IG(k_features=number_of_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_lr = make_pipeline(                        \n                        tfidf,\n                        ig,\n                        ComplementNB(alpha=alpha)\n                        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-Validation Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"####### Setting up Cross-Validation #####\nX=np.array(documents)\ny=np.array(classes)\nkfold = StratifiedKFold(n_splits=nfold,shuffle=True,random_state=1).split(X, y)\n\n####### Define Variables for Metrics #####\naccuracys = []\nscores= []\nmetriche = np.zeros((nfold,4,len(np.unique(classes))))\ntarget_names=list(map(str,np.unique(classes)))\n\n####### Cross-Validation Loop ############\n\nfor k, (train, test) in enumerate(kfold):\n\n    pipe_lr.fit(X[train], y[train])\n    y_pred=pipe_lr.predict(X[test])\n    y_prob=pipe_lr.predict_proba(X[test])\n\n    \n    ####### Compute Accuracy ##########\n    accuracy = pipe_lr.score(X[test], y[test])\n    accuracys.append(accuracy)\n    \n    ####### Compute Precision,Recall,F-Score ############\n    score=precision_recall_fscore_support(y_true=y[test], y_pred=y_pred, average=\"weighted\")\n    scores.append(score[0:3])\n    \n    print('--------------- Fold: %2d ---------------------'% (k+1))\n    print()\n    print(\"Accuracy:\",  round(accuracy,2))\n    print(\"Detail:\")\n    print(metrics.classification_report(y[test], y_pred))\n    \n    dizionario=metrics.classification_report(y[test], y_pred, target_names=target_names,output_dict=True)\n    for k_d,(m_id, m_info) in enumerate(dizionario.items()):\n        if k_d<len(np.unique(classes)):\n            for j_d,key in enumerate(m_info):\n                metriche[k,j_d,k_d]=m_info[key]\n        else:\n            break\n    \n    ## Plot confusion matrix\n    conf_mat = confusion_matrix(y[test], y_pred)\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(conf_mat, annot=True, fmt='d', ax=ax, cbar=False,cmap=plt.cm.Blues)\n    ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=target_names, \n           yticklabels=target_names, title=\"Confusion matrix\")\n    plt.yticks(rotation=0)\n    \n    print()\n    \n    plt.show()\n    \n    ## select observation\n    i = 0\n    txt_instance = X[test][i]\n    ## check true value and predicted value\n    print(\"True:\", y[test][i], \"--> Pred:\", y_pred[i], \"| Prob:\", round(np.max(y_prob[i]),2))\n    ## show explanation\n    explainer = lime_text.LimeTextExplainer(class_names=target_names)\n    explained = explainer.explain_instance(txt_instance, \n                 pipe_lr.predict_proba, num_features=6,top_labels=2)\n    explained.show_in_notebook(text=txt_instance, predict_proba=False)\n\n    \narr = np.array(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Overall results of the cross-validation procedure\")\nprint()\nprint(\"Level 1\")\nprint()\n\nprint('\\nCV accuracy: %.2f +/- %.2f max: %.2f' % (np.mean(accuracys)*100, np.std(accuracys)*100,np.max(accuracys)*100))\nprint('\\nCV precision: %.2f +/- %.2f max: %.2f' % (np.mean(arr[:,0])*100, np.std(arr[:,0])*100,np.max(arr[:,0])*100))\nprint('\\nCV recall: %.2f +/- %.2f max: %.2f' % (np.mean(arr[:,1])*100, np.std(arr[:,1])*100,np.max(arr[:,1])*100))\nprint('\\nCV f1: %.2f +/- %.2f max: %.2f' % (np.mean(arr[:,2])*100, np.std(arr[:,2])*100,np.max(arr[:,2])*100))\n\nprint()\nprint(\"Level 2\")\nprint()\nprint(f\"{'Class':^7} | {'precision':^9}{'':^6} | {'recall':^10}{'':^5} | {'f1-measure':^6}{'':^5} | {'support':^9}\")\nfor i in range(len(np.unique(classes))):\n    print(f\"{i :^7} | {np.mean(metriche[:,0,i])*100:^5.2f} +/-{np.std(metriche[:,0,i])*100:^6.2f} | {np.mean(metriche[:,1,i])*100:^5.2f} +/-{np.std(metriche[:,1,i])*100:^6.2f} | {np.mean(metriche[:,2,i])*100:^5.2f} +/-{np.std(metriche[:,2,i])*100:^6.2f} | {np.mean(metriche[:,3,i]):^9.2f}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}