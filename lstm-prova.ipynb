{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"all\")\nimport matplotlib.pyplot as plt\nimport torch\n\n%matplotlib inline","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Error loading all: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_excel('../input/politica/politica.xlsx', sheet_name=\"Foglio1\")\npd.options.display.float_format = '{:,.0f}'.format\nprint(f'Found {len(df)} texts.')\n\nprint(f'{df[\"cap_maj_master\"].isnull().sum()} document(s) with no classification removed')\ndf=df[pd.notnull(df['cap_maj_master'])]\n\nprint(f'{df[\"testo\"].isnull().sum()} document(s) with no text removed')\ndf=df[pd.notnull(df['testo'])]\n\nclasses = [int(c) for c in df['cap_maj_master']]\ndocuments = [d for d in df['testo']]\ndf = df[['cap_maj_master', 'testo']]\ndf.columns = ['sentiment', 'review']\ndf.head(3)\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"Found 5674 texts.\n2 document(s) with no classification removed\n424 document(s) with no text removed\n","name":"stdout"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   sentiment                                             review\n0          1  : quali siano le determinazioni del Governo in...\n1          1  : quali siano le valutazioni del Governo sugli...\n2          1  - premesso che: la prospettata modifica degli ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>: quali siano le determinazioni del Governo in...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>: quali siano le valutazioni del Governo sugli...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>- premesso che: la prospettata modifica degli ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\ndef preprocessor(text):\n    text = re.sub('<[^>]*>', ' ', str(text))\n    text=re.sub('\\d+',' ',str(text))\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           str(text))\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' +\n            ' '.join(emoticons).replace('-', ''))\n    return text\nimport unicodedata\n\ndef strip_accents(text):\n\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError: # unicode is a default on python 3 \n        pass\n\n    text = unicodedata.normalize('NFD', text)\\\n           .encode('ascii', 'ignore')\\\n           .decode(\"utf-8\")\n\n    return text\ndef tokenizer_porter(text):\n    stop=stopwords.words('italian')\n    stemmer = SnowballStemmer(\"italian\", ignore_stopwords=True)\n    return [stemmer.stem(word) for word in text.split()]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['review'] = df['review'].apply(preprocessor).apply(strip_accents).apply(tokenizer_porter)\ndf['sentiment']=df['sentiment'].astype(int)\ndf.head(3)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   sentiment                                             review\n0          1  [qual, siano, le, determin, del, govern, in, m...\n1          1  [qual, siano, le, valut, del, govern, sugli, e...\n2          1  [premess, che, la, prospett, modif, degli, sca...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[qual, siano, le, determin, del, govern, in, m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[qual, siano, le, valut, del, govern, sugli, e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[premess, che, la, prospett, modif, degli, sca...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df.iloc[:,1].values, df.iloc[:,0].values\nprint(X.shape)\nprint(y.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"(5248,)\n(5248,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torch\n# Use cuda if present\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device available for running: \")\nprint(device)","execution_count":6,"outputs":[{"output_type":"stream","text":"Device available for running: \ncuda\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom collections import defaultdict\n\ndef tokenize(texts):\n    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n    \n    Args:\n        texts (List[str]): List of text data\n    \n    Returns:\n        tokenized_texts (List[List[str]]): List of list of tokens\n        word2idx (Dict): Vocabulary built from the corpus\n        max_len (int): Maximum sentence length\n    \"\"\"\n\n    max_len = 0\n    tokenized_texts = []\n    word2idx = {}\n\n    # Add <pad> and <unk> tokens to the vocabulary\n    word2idx['<pad>'] = 0\n    word2idx['<unk>'] = 1\n\n    # Building our vocab from the corpus starting from index 2\n    idx = 2\n    for sent in texts:\n        \n\n        # Add `tokenized_sent` to `tokenized_texts`\n        tokenized_texts.append(sent)\n\n        # Add new token to `word2idx`\n        for token in sent:\n            if token not in word2idx:\n                word2idx[token] = idx\n                idx += 1\n\n        # Update `max_len`\n        max_len = max(max_len, len(sent))\n\n    return tokenized_texts, word2idx, max_len\n\ndef encode(tokenized_texts, word2idx, max_len):\n    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n    their index in the vocabulary.\n\n    Returns:\n        input_ids (np.array): Array of token indexes in the vocabulary with\n            shape (N, max_len). It will the input of our CNN model.\n    \"\"\"\n\n    input_ids = []\n    for tokenized_sent in tokenized_texts:\n        # Pad sentences to max_len\n        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n\n        # Encode tokens to input_ids\n        input_id = [word2idx.get(token) for token in tokenized_sent]\n        input_ids.append(input_id)\n    \n    return np.array(input_ids)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\n\ndef load_pretrained_vectors(word2idx, fname):\n    \"\"\"Load pretrained vectors and create embedding layers.\n    \n    Args:\n        word2idx (Dict): Vocabulary built from the corpus\n        fname (str): Path to pretrained vector file\n\n    Returns:\n        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n            the size of word2idx and d is embedding dimension\n    \"\"\"\n\n    print(\"Loading pretrained vectors...\")\n    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    n, d = map(int, fin.readline().split())\n\n    # Initilize random embeddings\n    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n    embeddings[word2idx['<pad>']] = np.zeros((d,))\n\n    # Load pretrained vectors\n    count = 0\n    for line in tqdm_notebook(fin):\n        tokens = line.rstrip().split(' ')\n        word = tokens[0]\n        if word in word2idx:\n            count += 1\n            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n\n    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n\n    return embeddings","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenized_texts, word2idx, max_len = tokenize(X)\ninput_ids = encode(tokenized_texts, word2idx, max_len)\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pretrained vectors\nprint(\"Tokenizing...\\n\")\nembeddings = load_pretrained_vectors(word2idx, \"../input/fasttext-aligned-word-vectors/wiki.it.align.vec\")\nembeddings = torch.tensor(embeddings)","execution_count":10,"outputs":[{"output_type":"stream","text":"Tokenizing...\n\nLoading pretrained vectors...\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd185f4f16e4f649da674d7fcca0db4"}},"metadata":{}},{"output_type":"stream","text":"\nThere are 9206 / 25951 pretrained vectors found.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embeddings)","execution_count":11,"outputs":[{"output_type":"stream","text":"tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0175, -0.1566,  0.1571,  ...,  0.0798, -0.1978, -0.0766],\n        [-0.0121,  0.0414,  0.0418,  ..., -0.0653, -0.0301,  0.0157],\n        ...,\n        [ 0.0908, -0.0678,  0.2452,  ...,  0.0368,  0.0007, -0.0125],\n        [-0.0009, -0.0082,  0.0163,  ..., -0.0984, -0.0577,  0.0485],\n        [-0.0719, -0.0453, -0.1825,  ..., -0.0982,  0.0997, -0.0467]],\n       dtype=torch.float64)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_ids)","execution_count":12,"outputs":[{"output_type":"stream","text":"[[  2   3   4 ...   0   0   0]\n [  2   3   4 ...   0   0   0]\n [ 59  60  61 ...   0   0   0]\n ...\n [ 59  60  71 ...   0   0   0]\n [ 59  60   8 ...   0   0   0]\n [ 59  60 790 ...   0   0   0]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n                              SequentialSampler)\n\ndef data_loader(train_inputs, val_inputs, train_labels, val_labels,\n                batch_size=50):\n    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n    DataLoader.\n    \"\"\"\n\n    # Convert data type to torch.Tensor\n    train_inputs, val_inputs, train_labels, val_labels =\\\n    tuple(torch.tensor(data) for data in\n          [train_inputs, val_inputs, train_labels, val_labels])\n\n    # Specify batch_size\n    batch_size = 50\n\n    # Create DataLoader for training data\n    train_data = TensorDataset(train_inputs, train_labels)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n    # Create DataLoader for validation data\n    val_data = TensorDataset(val_inputs, val_labels)\n    val_sampler = SequentialSampler(val_data)\n    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n    \n    return train_dataloader, val_dataloader","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSTM_NLP(nn.Module):\n    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n    def __init__(self,\n                 pretrained_embedding=None,\n                 freeze_embedding=False,\n                 vocab_size=None,\n                 embed_dim=300,\n                 hidden_dim=192,\n                 output_dim=1,\n                 n_layers=2,\n                 bidirectional=True,\n                 dropout=0.2,\n                 batch_size=50):\n        \"\"\"\n        The constructor for LSTM_NLP class.\n\n        Args:\n            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n                shape (vocab_size, embed_dim)\n            freeze_embedding (bool): Set to False to fine-tune pretraiend\n                vectors. Default: False\n            vocab_size (int): Need to be specified when not pretrained word\n                embeddings are not used.\n            embed_dim (int): Dimension of word vectors. Need to be specified\n                when pretrained word embeddings are not used. Default: 300\n            \n            dropout (float): Dropout rate. Default: 0.5\n        \"\"\"\n\n        super(LSTM_NLP, self).__init__()\n        # Embedding layer\n        if pretrained_embedding is not None:\n            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n                                                          freeze=freeze_embedding)\n        else:\n            self.embed_dim = embed_dim\n            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n                                          embedding_dim=self.embed_dim,\n                                          padding_idx=0,\n                                          max_norm=5.0)\n        # LSTM\n        ###### chiedere qui al prof\n        self.rnn = nn.LSTM(self.embed_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.linear = nn.Linear(hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x_ = self.embedding(x.long()).float()\n        x_, (h_n, c_n) = self.rnn(x_)\n        x_ = (x_[:, -1, :])\n        x_ = self.fc1(x_)\n        return x_","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\ndef initilize_model(pretrained_embedding=None,\n                    freeze_embedding=False,\n                    vocab_size=None,\n                    embed_dim=300,\n                    hidden_dim=300,\n                    output_dim=1,\n                    n_layers=2,\n                    dropout=0.2,\n                    learning_rate=0.01):\n    \"\"\"Instantiate a LSTM model and an optimizer.\"\"\"\n\n    \n\n    # Instantiate CNN model\n    lstm_model = LSTM_NLP(pretrained_embedding=pretrained_embedding,\n                        freeze_embedding=freeze_embedding,\n                        vocab_size=vocab_size,\n                        embed_dim=embed_dim,\n                        hidden_dim=hidden_dim,\n                        output_dim=output_dim,\n                        n_layers=n_layers,\n                        dropout=dropout)\n    \n    # Send model to `device` (GPU/CPU)\n    lstm_model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    criterion=criterion.to(device)\n\n    # Instantiate Adadelta optimizer\n    optimizer = optim.Adadelta(lstm_model.parameters(),\n                               lr=learning_rate,\n                               rho=0.95)\n\n    return lstm_model, optimizer, criterion ","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport time\n\n\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, optimizer,criterion, train_dataloader, val_dataloader=None, epochs=10):\n    \"\"\"Train the LSTM model.\"\"\"\n    \n    # Tracking best validation accuracy\n    best_accuracy = 0\n\n    # Start training loop\n    print(\"Start training...\\n\")\n    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n    print(\"-\"*60)\n    countdown=epochs\n    count=1\n    while countdown>0:\n        # =======================================\n        #               Training\n        # =======================================\n\n        # Tracking time and loss\n        t0_epoch = time.time()\n        total_loss = 0\n\n        # Put the model into the training mode\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            # Load batch to GPU\n            \n            b_input_ids, b_labels = tuple(t.to(device).long() for t in batch)\n            \n            # Zero out any previously calculated gradients\n            model.zero_grad()\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids)\n\n            # Compute loss and accumulate the loss values\n            loss = criterion(logits, b_labels)\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Update parameters\n            optimizer.step()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if val_dataloader is not None:\n            # After the completion of each training epoch, measure the model's\n            # performance on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader,criterion)\n\n            # Track the best accuracy\n            if val_accuracy > best_accuracy:\n                best_accuracy = val_accuracy\n                torch.save(model.state_dict(), 'tut5-model.pt')\n                countdown=epochs\n            else:\n                countdown=countdown-1\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            print(f\"{count + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            count=count+1\n            \n    print(\"\\n\")\n    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n    return best_accuracy\n\ndef evaluate(model, val_dataloader,criterion):\n    \"\"\"After the completion of each training epoch, measure the model's\n    performance on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled\n    # during the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_labels = tuple(t.to(device).long() for t in batch)\n        \n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids)\n\n        # Compute loss\n        loss = criterion(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text, model, max_len=62):\n    \"\"\"Predict probability that a review is positive.\"\"\"\n    labels=[]\n    # Tokenize, pad and encode text\n    for sent in text:\n        model.eval()\n        tokenized=tokenizer_porter(strip_accents(preprocessor(sent)))\n        if len(tokenized) < min_len:\n            tokenized += ['<pad>'] * (min_len - len(tokenized))\n        indexed = [word2idx.get(token, word2idx['<unk>']) for token in tokenized]\n         \n        tensor = torch.LongTensor(indexed).to(device)\n        tensor = tensor.unsqueeze(1)\n        preds = model(tensor)\n        max_preds = preds.argmax(dim = 1)\n        label.append(max_preds.item())\n    return labels\ndef predictTen(text, model, max_len=62):\n    \"\"\"Predict probability that a review is positive.\"\"\"\n    labels=[]\n    # Tokenize, pad and encode text\n    for input_id in text:\n        \n        input_id = torch.tensor(input_id).to(device).unsqueeze(dim=0)\n        # Compute logits\n        logits = model.forward(input_id)\n        preds = logits.argmax(dim=1)\n        #  Compute probability\n        labels.append(preds.item())\n        #  Compute probability\n        \n\n    return labels","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\nX=input_ids\ny=y\nnumclass=len(np.unique(y))+3\nnfold=2\nkfold = StratifiedKFold(n_splits=nfold).split(input_ids, y)\n\naccuracys = []\nscores= []\n\ntarget_names=list(np.unique(y))\n\nfor k, (tr, test) in enumerate(kfold):\n    train_dataloader, val_dataloader =data_loader(X[tr], X[test], y[tr], y[test], batch_size=50)\n    \n    \n    \n    set_seed(42)\n    lstm_model, optimizer, criterion = initilize_model(pretrained_embedding=embeddings,\n                                            output_dim=numclass,\n                                            freeze_embedding=False,\n                                            learning_rate=0.25,                                            \n                                            dropout=0.2)\n    accuracy=train(lstm_model, optimizer, criterion, train_dataloader, val_dataloader, epochs=5)\n    accuracys.append(accuracy)\n    lstm_model.load_state_dict(torch.load('tut5-model.pt'))\n    y_pred=predictTen(X[test],lstm_model,max_len=max_len)\n    \n    score=precision_recall_fscore_support(y_true=y[test], y_pred=y_pred, labels=np.unique(y_pred), average=\"weighted\")\n    scores.append(score[0:3])\n    print('--------------- Fold: %2d ---------------------'% (k+1))\n    print()\n    target_names = list(map(str,target_names))\n    print(metrics.classification_report(y[test], y_pred, target_names=target_names))\n    conf_mat = confusion_matrix(y[test], y_pred)\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(conf_mat, annot=True, fmt='d',\n                xticklabels=target_names , yticklabels=target_names )\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()\n    print()\n\narr = np.array(scores)\n\nprint(\"Overall results of the cross-validation procedure\")\nprint()\n\nprint('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracys), np.std(accuracys)))\nprint('\\nCV precision: %.3f +/- %.3f' % (np.mean(arr[:,0]), np.std(arr[:,0])))\nprint('\\nCV recall: %.3f +/- %.3f' % (np.mean(arr[:,1]), np.std(arr[:,1])))\nprint('\\nCV f1: %.3f +/- %.3f' % (np.mean(arr[:,2]), np.std(arr[:,2])))\n    \n\n","execution_count":18,"outputs":[{"output_type":"stream","text":"Start training...\n\n Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n------------------------------------------------------------\n   2    |   4.596403   |  3.060827  |   13.06   |   31.21  \n   3    |   2.926502   |  2.973368  |   13.62   |   31.14  \n   4    |   2.883952   |  2.939296  |   9.51    |   30.97  \n   5    |   2.871156   |  2.866077  |   13.62   |   30.99  \n   6    |   2.870182   |  2.882198  |   9.25    |   30.97  \n   7    |   2.858594   |  2.893022  |   7.51    |   31.03  \n   8    |   2.850756   |  2.860109  |   10.26   |   30.97  \n\n\nTraining complete! Best accuracy: 13.62%.\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'cnn_non_static' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-bf335c37c3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0maccuracys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tut5-model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictTen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn_non_static\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"weighted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cnn_non_static' is not defined"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}