{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom torchtext import datasets\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\n\ntorch.backends.cudnn.deterministic = True","execution_count":159,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 123\ntorch.manual_seed(RANDOM_SEED)\n\nVOCABULARY_SIZE = 5000\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 128\nNUM_EPOCHS = 50\nDROPOUT = 0.5\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nEMBEDDING_DIM = 128\nBIDIRECTIONAL = True\nHIDDEN_DIM = 192\nNUM_LAYERS = 2\nOUTPUT_DIM = 23","execution_count":160,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('../input/ciao9cci/politica.xlsx', sheet_name=\"Foglio1\")\npd.options.display.float_format = '{:,.0f}'.format\nprint(f'Found {len(df)} texts.')\n\nprint(f'{df[\"cap_maj_master\"].isnull().sum()} document(s) with no classification removed')\ndf=df[pd.notnull(df['cap_maj_master'])]\n\nprint(f'{df[\"testo\"].isnull().sum()} document(s) with no text removed')\ndf=df[pd.notnull(df['testo'])]\n\nclasses = [int(c) for c in df['cap_maj_master']]\ndocuments = [d for d in df['testo']]\ndf = df[['cap_maj_master', 'testo']]\ndf.columns = ['classlabel', 'content']\ndf['classlabel'] = df['classlabel']-1\ndf.classlabel = df.classlabel.astype(int)\ndf.head(3)","execution_count":161,"outputs":[{"output_type":"stream","text":"Found 5674 texts.\n2 document(s) with no classification removed\n424 document(s) with no text removed\n","name":"stdout"},{"output_type":"execute_result","execution_count":161,"data":{"text/plain":"   classlabel                                            content\n0           0  : quali siano le determinazioni del Governo in...\n1           0  : quali siano le valutazioni del Governo sugli...\n2           0  - premesso che: la prospettata modifica degli ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classlabel</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>: quali siano le determinazioni del Governo in...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>: quali siano le valutazioni del Governo sugli...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>- premesso che: la prospettata modifica degli ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(df['classlabel'].values)","execution_count":162,"outputs":[{"output_type":"execute_result","execution_count":162,"data":{"text/plain":"array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 22])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(np.bincount(df['classlabel']))","execution_count":163,"outputs":[{"output_type":"execute_result","execution_count":163,"data":{"text/plain":"23"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize \ndef preprocessor(text):\n    text = re.sub('<[^>]*>', ' ', str(text))\n    text=re.sub('\\d+',' ',str(text))\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           str(text))\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' +\n            ' '.join(emoticons).replace('-', ''))\n    return text\nimport unicodedata\ndef strip_accents(text):\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError: # unicode is a default on python 3 \n        pass\n\n    text = unicodedata.normalize('NFD', text)\\\n           .encode('ascii', 'ignore')\\\n           .decode(\"utf-8\")\n    return text\ndef tokenizer_porter(text):\n    stop=set(stopwords.words('italian'))\n    word_tokens = word_tokenize(text)\n    filtered_sentence = [w for w in word_tokens if not w in stop]\n    \n    stemmer = SnowballStemmer(\"italian\", ignore_stopwords=True)\n    return [stemmer.stem(word) for word in filtered_sentence]","execution_count":164,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['content']=df.content.apply(preprocessor).apply(strip_accents)\ndf.head(3)","execution_count":165,"outputs":[{"output_type":"execute_result","execution_count":165,"data":{"text/plain":"   classlabel                                            content\n0           0   quali siano le determinazioni del governo in ...\n1           0   quali siano le valutazioni del governo sugli ...\n2           0   premesso che la prospettata modifica degli sc...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classlabel</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>quali siano le determinazioni del governo in ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>quali siano le valutazioni del governo sugli ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>premesso che la prospettata modifica degli sc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['classlabel', 'content']].to_csv('./train_prepocessed.csv', index=None)","execution_count":166,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df","execution_count":167,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = data.Field(sequential=True,\n                  tokenize='spacy',\n                  include_lengths=True) # necessary for packed_padded_sequence\n\nLABEL = data.LabelField(dtype=torch.float)","execution_count":168,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fields = [('classlabel', LABEL), ('content', TEXT)]\n\ntrain_dataset = data.TabularDataset(\n    path=\"./train_prepocessed.csv\", format='csv',\n    skip_header=True, fields=fields)\n\n","execution_count":169,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, valid_data = train_dataset.split(\n    split_ratio=[0.95, 0.05],\n    random_state=random.seed(RANDOM_SEED))\n\nprint(f'Num Train: {len(train_data)}')\nprint(f'Num Valid: {len(valid_data)}')","execution_count":170,"outputs":[{"output_type":"stream","text":"Num Train: 4986\nNum Valid: 262\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT.build_vocab(train_data,\n                 max_size=VOCABULARY_SIZE,\n                 vectors='fasttext.simple.300d',\n                 unk_init=torch.Tensor.normal_)\n\nLABEL.build_vocab(train_data)\n\nprint(f'Vocabulary size: {len(TEXT.vocab)}')\nprint(f'Number of classes: {len(LABEL.vocab)}')","execution_count":171,"outputs":[{"output_type":"stream","text":"Vocabulary size: 5002\nNumber of classes: 21\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader, valid_loader = data.BucketIterator.splits(\n    (train_data, valid_data), \n    batch_size=BATCH_SIZE,\n    sort_within_batch=True, # necessary for packed_padded_sequence\n    sort_key=lambda x: len(x.content),\n    device=DEVICE)","execution_count":172,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.content[0].size()}')\n    print(f'Target vector size: {batch.classlabel.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.content[0].size()}')\n    print(f'Target vector size: {batch.classlabel.size()}')\n    break\n    \n","execution_count":173,"outputs":[{"output_type":"stream","text":"Train\nText matrix size: torch.Size([445, 128])\nTarget vector size: torch.Size([128])\n\nValid:\nText matrix size: torch.Size([281, 128])\nTarget vector size: torch.Size([128])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.nn as nn\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_dim, embedding_dim, bidirectional, hidden_dim, num_layers, output_dim, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim,\n                           num_layers=num_layers,\n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        self.fc1 = nn.Linear(hidden_dim * num_layers, 64)\n        self.fc2 = nn.Linear(64, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_length):\n\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        hidden = self.fc1(hidden)\n        hidden = self.dropout(hidden)\n        hidden = self.fc2(hidden)\n        return hidden","execution_count":174,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIM = len(TEXT.vocab)\n\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\ntorch.manual_seed(RANDOM_SEED)\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, BIDIRECTIONAL, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM, DROPOUT, PAD_IDX)\nmodel = model.to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\ncriterion=criterion.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":176,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, data_loader, criterion):\n    model.eval()\n    val_accuracy = []\n    val_loss = []\n    with torch.no_grad():\n        for batch_idx, batch_data in enumerate(data_loader):\n            text, text_lengths = batch_data.content\n            logits = model(text, text_lengths)\n            loss = criterion(logits, batch_data.classlabel.long())\n            val_loss.append(loss.item())\n            \n            _, preds = torch.max(logits, 1)\n            accuracy = (preds == batch_data.classlabel.long()).cpu().numpy().mean() * 100\n            val_accuracy.append(accuracy)\n        val_loss = np.mean(val_loss)\n        val_accuracy = np.mean(val_accuracy)\n        return val_loss, val_accuracy","execution_count":177,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train(model, optimizer,criterion, train_dataloader, val_dataloader=None, epochs=10):\n    \"\"\"Train the LSTM model.\"\"\"\n    \n    # Tracking best validation accuracy\n    best_accuracy = 0\n\n    # Start training loop\n    print(\"Start training...\\n\")\n    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n    print(\"-\"*60)\n    countdown=epochs\n    count=1\n    while countdown>0:\n        # =======================================\n        #               Training\n        # =======================================\n\n        # Tracking time and loss\n        t0_epoch = time.time()\n        total_loss = 0\n        model.train()\n        step=1\n        for batch_idx, batch_data in enumerate(train_loader):\n            text, text_lengths = batch_data.content\n        \n            ### FORWARD AND BACK PROP\n            logits = model(text, text_lengths)\n            loss = criterion(logits, batch_data.classlabel.long())\n            total_loss += loss.item()\n            optimizer.zero_grad()\n            step=step+1\n            loss.backward()\n        \n            ### UPDATE MODEL PARAMETERS\n            optimizer.step()\n        \n        avg_train_loss = total_loss / len(train_dataloader)\n        \n        val_loss, val_accuracy = evaluate(model, val_dataloader,criterion)\n        \n        # Track the best accuracy\n        if val_accuracy >= best_accuracy:\n            best_accuracy = val_accuracy\n            torch.save(model.state_dict(), 'tut5-model.pt')\n            countdown=epochs\n        else:\n            countdown=countdown-1\n        time_elapsed = time.time() - t0_epoch\n        print(f\"{count :^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n        count=count+1\n    print(\"\\n\")\n    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n    return best_accuracy\n    \n        ","execution_count":178,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(model, optimizer,criterion, train_loader, valid_loader, 10)","execution_count":null,"outputs":[{"output_type":"stream","text":"Start training...\n\n Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n------------------------------------------------------------\n   1    |   2.956469   |  2.831449  |   9.64    |   10.38  \n   2    |   2.865184   |  2.843997  |   9.64    |   10.41  \n   3    |   2.832933   |  2.819625  |   9.64    |   10.41  \n   4    |   2.800553   |  2.835640  |   9.90    |   10.35  \n   5    |   2.770646   |  2.822638  |   11.98   |   10.54  \n   6    |   2.705731   |  2.721217  |   13.80   |   10.38  \n   7    |   2.644482   |  2.703880  |   13.80   |   10.33  \n   8    |   2.589377   |  2.622101  |   15.10   |   10.37  \n   9    |   2.512120   |  2.724893  |   21.18   |   10.34  \n  10    |   2.429830   |  2.536421  |   22.74   |   10.41  \n  11    |   2.396723   |  2.568830  |   24.57   |   10.30  \n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}